# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kt28xGkk-nwAlltHaGcZdrktoOd61i6U
"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('not_wyckoff_data.csv')

# Prepare to store the normalized sequences
normalized_sequences = []

# Get the number of 37-row sequences in the data
num_sequences = len(df) // 37

# For each 37-row sequence
for i in range(num_sequences):
    # Select the sequence
    sequence = df.iloc[i*37:(i+1)*37, :]

    # Normalize the OHLC data for this sequence
    scaler = MinMaxScaler()
    normalized_sequence = scaler.fit_transform(sequence)

    # Add the normalized sequence to the list
    normalized_sequences.append(normalized_sequence)

# Combine all the sequences back into a single numpy array
normalized_data = np.concatenate(normalized_sequences)

normalized_data

def add_noise(sequence, noise_factor=0.05):
    sequence_copy = sequence.copy()
    noise = np.random.normal(0, noise_factor, sequence_copy.shape)
    sequence_copy += noise
    sequence_copy = np.clip(sequence_copy, 0, 1)
    return sequence_copy

# Calculate number of augmentations needed per sequence to get at least 9990 rows
num_augmentations = int(np.ceil((9990 / 37 - num_sequences) / num_sequences))

# Initialize list to store augmented data
augmented_data = []

# Add noise and augment each sequence
for sequence in normalized_sequences:
    augmented_data.append(sequence)
    for _ in range(num_augmentations):
        noise_sequence = add_noise(sequence)
        augmented_data.append(noise_sequence)

# Convert list to numpy array and reshape to ensure it's a 2D array
augmented_data = np.array(augmented_data).reshape(-1, 37, sequence.shape[1])

# If we ended up with more than 9990 rows, truncate the excess
if len(augmented_data) > 9990:
    augmented_data = augmented_data[:9990]

print(augmented_data.shape)
type(augmented_data)

non_wyckoff_data = np.array(augmented_data)

print(non_wyckoff_data.shape)
type(non_wyckoff_data)

non_wyckoff_data

import matplotlib.pyplot as plt

# Select the first sequence
sequence = augmented_data[0]

# Create a new figure
plt.figure(figsize=(12, 6))

# Plot each feature
for i in range(sequence.shape[1]):
    plt.plot(sequence[:, i])

plt.title('Augmented Sequence')
plt.show()

df_wyckoff = pd.read_csv('/content/padded_sequences_augmented_final_data.csv')


# Convert the DataFrame values to floating point numbers
df_wyckoff = df_wyckoff.astype(float)

print(df_wyckoff.shape)

num_patterns = len(df_wyckoff) // 37
wyckoff_sequences = np.array(df_wyckoff).reshape((num_patterns, 37, 5))

print(wyckoff_sequences.shape)  # This should print: (168, 37, 5)

print(wyckoff_sequences.shape)
type(wyckoff_sequences)

wyckoff_labels = np.ones((wyckoff_sequences.shape[0],))
non_wyckoff_labels = np.zeros((non_wyckoff_data.shape[0],))

X = np.concatenate((wyckoff_sequences, non_wyckoff_data))
y = np.concatenate((wyckoff_labels, non_wyckoff_labels))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.utils import shuffle

# Shuffle the data and labels
X, y = shuffle(X, y, random_state=42)

# Then split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Define model architecture
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(37, 5)))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))

# Evaluate the model
scores = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

# Make predictions on the test set
y_pred = model.predict(X_test)

# Convert probabilities into binary outputs
y_pred = (y_pred > 0.5).astype(int)

# Calculate metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Precision: %.2f" % precision)
print("Recall: %.2f" % recall)
print("F1 score: %.2f" % f1)
print("ROC AUC score: %.2f" % roc_auc)

model.save('model.h5')